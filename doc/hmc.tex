%\documentclass{article}
\documentclass[notitlepage, preprint,superscriptaddress]{revtex4-1}



\usepackage{amsmath}
\usepackage{bm}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{subfigure}
\begin{document}




\newcommand{\vct}[1]{\bm{\mathrm{#1}}}
\newcommand{\vx}{\vct{x}}
\newcommand{\vy}{\vct{y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Ham}{\mathcal{H}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\A}{\mathcal{A}}

% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{OliveGreen}\small [\textbf{Comment.} #1]}}

\newcommand{\hl}[1]{{\color{red} #1}}



\section{Method}

\subsection{Hybrid Monte Carlo (HMC)}



The hybrid Monte Carlo (HMC)
is a technique to incorporate Metropolis acceptance
into a molecular dynamics (MD) simulation.
%
Here, we use it to sample a flat distribution
along an arbitrary quantity $A$.
%
The method
does not require $A$ to be differentiable,
which makes it applicable to
a complex reaction coordinate.

Consider a canonical ensemble,
the unbiased distribution
is given by
\begin{align}
  \rho_0(A)
=
  \int
    \delta[A - \mathcal A(\vct{r})]
    \, \frac{ e^{-\beta U(\vct{r})} } { Z } \, d\vct{r},
\end{align}
where,
$\beta = 1/(k_B T)$ is the inverse temperature,
$U(\vct{r})$ is potential energy function,
$Z = \int e^{-\beta U(\vct{r})} \, d\vct{r}$ is the partition function,
and $\delta(\dots)$ is $\delta$-function.
%
One can readily verify that this distribution is properly normalized as
\[
  \int \rho_0(A) \, dA = 1.
\]
%
If we run a regular MC or MD simulation in the canonical ensemble,
the distribution of $A$ is given by $\rho_0(A)$.

Let us now consider the sampling of a biased distribution
\begin{equation}
  \rho(A) \propto \rho_0(A) \, e^{-w(A)}.
\label{eq:rhoQbiased}
\end{equation}
This can be accomplished by HMC by repeating the following steps
\begin{enumerate}
  \item Perform $k$ MD steps with a stochastic thermostat,
        and let the system evolute
        from the state $(\vct{r}, \vct v)$ to $(\vct{r}', \vct v')$.
        We set $k = 1$ in this study.

  \item Compute $\Delta w \equiv w[\mathcal A(\vct{r}')] - w[\mathcal A(\vct{r})]$,
        and generate a uniform random number $r$ between $0$ and $1$.

  \item If $r < \min\{e^{-\Delta w}, 1\}$, do nothing,
        that is, remain in the state $(\vct{r}', \vct v')$.
        Otherwise, change the current state to $(\vct{r}, -\vct v)$
        (notice the reversal of velocity).
\end{enumerate}




One can show that above algorithm satisfies detailed balance,
and gives the desired distribution Eq. \eqref{eq:rhoQbiased}.
%
Its efficiency, however, depends on the randomness of the thermostat.
%
One can apply an additional step at the end of the above algorithm
to increase the randomness.
%
For example, we can randomly swap two the velocity of two atoms:
\[
  (\vct v_i, \vct v_j)
 \rightarrow
  \left(
     \sqrt{ \frac{ m_j } {m_i}  } \vct v_j,
     \sqrt{ \frac{ m_i } {m_j}  } \vct v_i
  \right).
\]
Note that if such a swap is used,
the degree of freedom of system should be changed as well:
if the masses are uniform,
only the three linear but not the angular momenta are conserved;
if the masses are not uniform,
no global momentum is conserved.



\subsection{Sampling a flat distribution}



We now pursue a special case of Eq. \eqref{eq:rhoQbiased},
that is, a flat distribution of $A$.
\[
  \rho(A) = \mbox{constant},
\]
which requires $w(A) = \ln \rho_0(A) + \mbox{constant}$.


To do so, we used the Wang-Landau algorithm\cite{
wang2001, wang2001pre}.
%
We first step up a few bins along $A$:
$(A_i, A_{i+1})$,
and start from a zero bias potential
$w_i \equiv 0$.
%
Every time the system visits bin $i$,
we increase the bias potential there as\cite{
wang2001, wang2001pre}
%
\begin{equation}
w_i \rightarrow w_i + \ln f.
\label{eq:wlupdate}
\end{equation}



Equation \eqref{eq:wlupdate} encourages
the system to move out of bin $i$.
%
Using such an updating scheme,
one can sample a flat distribution along $A$.
%
The error of $w(A)$, however, increases with
the updating factor $\ln f$.
%
Thus, once a relative flat histogram is achieved
by a certain $\ln f$, we reduce the updating factor
by half\cite{
wang2001, wang2001pre}
%
\begin{equation}
  \ln f \rightarrow \ln f / 2.
\label{eq:lnfupdate}
\end{equation}
%
Asymptotically, it was shown that
Eq. \eqref{eq:lnfupdate} reduces $\ln f$ too quickly,
and a better formula to ensure convergence is\cite{
belardinelli2007, *belardinelli2008}
\begin{equation}
  \ln f = C / t,
\end{equation}
where $t$ is the number of simulation steps per bin,
and $C$ is an empirical constant
(which is often set to unity in MC simulations,
and according to our experience, should be larger for HMC).



\bibliography{simul}
\end{document}
